{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d73132b",
   "metadata": {},
   "source": [
    "<h1>ECON 140R Class 17</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25031a3e",
   "metadata": {},
   "source": [
    "<b>Spurious correlation</b> is a big problem in time series econometrics. It describes a situation where two variables appear to be correlated for completely accidental reasons, and our statistical methods may not allow us to correct the picture easily. Let us explore a hilariously weird example in time series data provided by [Tyler Vigen on his website](https://tylervigen.com/spurious-correlations). Along the way, we will examine some time series commands in __R__.\n",
    "\n",
    "Learning objectives:\n",
    "\n",
    "1. Time series regressions can be `lm()` regressions\n",
    "2. In __R__, you can also use the `dynlm` package and `dynlm()` after a data transform with `ts()`\n",
    "3. Spurious correlation is hilarious and potentially pernicious and persistent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b628cf",
   "metadata": {},
   "source": [
    "Below we load in `haven` like usual, and we also load in `dynlm` to run time-series models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac256e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(haven)\n",
    "install.packages(\"dynlm\")\n",
    "library(dynlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c7b05",
   "metadata": {},
   "source": [
    "<h2>A preposterious relationship ...</h2>\n",
    "\n",
    "The data in `spurious.dta` are hilariously weird. Inspired by Tyler Vigen's example, I found two time series from separate sources: the divorce rate per 1,000 in the state of Maine, `divorceme`, from CDC vital statistics; and the national per capita consumption of margarine in pounds per person (per year). Both series are annual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d913c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spurious <- read_dta(\"spurious.dta\")\n",
    "spurious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b4531",
   "metadata": {},
   "source": [
    "The __R__ package `dynlm` contains some special tools for dealing with time series, and `ts()` is one of them. The convention appears to be that we should exclude the time variable itself, in this case `year`, if we want to run things easily like `plot` with the dataset.\n",
    "\n",
    "Florian Heiss writes a little about this package in [Section 10.3](http://www.urfie.net/read/index.html#page/201), which mirrors the latest edition of Wooldridge, if you have access to that. Heiss does not go into great detail, but you can still get big-picture takeaways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b301dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's omit the year column by looking at columns 2:3\n",
    "# The start field is the beginning year, and frequency = 1 means annual data.\n",
    "spurious_ts <- ts(spurious[c(2:3)], start = 1999, frequency = 1)\n",
    "head(spurious_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93833419",
   "metadata": {},
   "source": [
    "It's never a bad idea to look at a scatter plot of a $y$ versus an $x$. The choice of either one is ... totally silly in this context, but for kicks, let us imagine that the divorce rate in Maine ($y$) is caused by the per capita consumption of margarine ($x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55262b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(spurious$margarine, spurious$divorceme)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affff815",
   "metadata": {},
   "source": [
    "It is also common to look at time series along a plot with time as the $x$-variable, and the time series as two (or more) different series graphed against time.\n",
    "\n",
    "Here is a nifty trick to overlay two time series on a time plot using the built-in `plot()` command, using a call to `lines()` afterward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(spurious$year, spurious$margarine, type = \"l\", col = \"red\")\n",
    "lines(spurious$year, spurious$divorceme, col = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5578dc",
   "metadata": {},
   "source": [
    "This picture is not quite as evocative as Tyler Vigen's, because Tyler introduced a secondary $y$-axis with its own scaling. This is visible if you look closely at the left and right sides of the original graph, reprinted for kicks below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1f6fc",
   "metadata": {},
   "source": [
    "<img src=\"images/divorce_margarine.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16837aa8",
   "metadata": {},
   "source": [
    "Part of the `dynlm` package appears to organize variables so that calls to `plot()` produce reasonable pictures. If you want to lose your mind, try replacing \"spurious_ts\" with \"spurious\" below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(spurious_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac47ece",
   "metadata": {},
   "source": [
    "<h2>... can still be statistically significant</h2>\n",
    "    \n",
    "Ready for some hilarious high jinks?\n",
    "\n",
    "There is nothing stopping us from running `lm()` to estimate the scatter plot relationship that we saw earlier, pretending in our heads that it is not preposterous to argue that national margarine consumption causes the divorce rate in Maine. (It is also preposterous to propose the reverse; I just chose one and went with it.)\n",
    "\n",
    "Below, we run this regression using `lm()`:\n",
    "\n",
    "$$ \n",
    "divorceme_t = \\alpha + \\beta \\ margarine_t + \\epsilon_t\n",
    "$$\n",
    "\n",
    "where now the observations are across $t$ for time, rather than units like people or households. Bombs away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d98372",
   "metadata": {},
   "outputs": [],
   "source": [
    "spurious_reg1 <- lm(divorceme ~ margarine, data = spurious)\n",
    "summary(spurious_reg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9cafd",
   "metadata": {},
   "source": [
    "This is all preposterous. What the model purports to say is that for every additional pound of annual margarine consumption, there is an increase in the divorce rate per 1,000 of $\\beta = 0.2$. The constant term $\\alpha$ tells us what the divorce rate is predicted to be if margarine consumption is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30046d3e",
   "metadata": {},
   "source": [
    "Often, a good solution is to introduce a <b>time trend</b> into the regression. The idea is that when $y$ and $x$ are both trending, modeling a time trend might allow ordinary least squares to let the time trend \"win out\" in a horse race for explaining $y$, removing the spurious correlation. The math would become something like this:\n",
    "\n",
    "$$ \n",
    "divorceme_t = \\alpha + \\beta \\ margarine_t + \\delta \\ t + \\epsilon_t\n",
    "$$\n",
    "\n",
    "where the time trend is $\\delta$ and $t$ indexes the time periods. For us, $t$ indexes years.\n",
    "\n",
    "Unfortunately for us, in this case a time trend does not seem to help at all. Here is how you could code it with `lm()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "spurious_reg2 <- lm(divorceme ~ margarine + year, data = spurious)\n",
    "summary(spurious_reg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f995fa",
   "metadata": {},
   "source": [
    "Note how the intercept is very different now. This is because we have tossed in the variable `year`, which ranges between 1999 and 2010. The reason the intercept is so large and negative is because we only get just the constant term for the out-of-sample prediction for the year 0. \n",
    "\n",
    "(Even worse, apparently there was no year 0. That is why the new millennium apparently began on January 1, 2001. Drat these infernal calendars.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a73ac",
   "metadata": {},
   "source": [
    "<h2>dynlm( ) looks like lm( )</h2>\n",
    "\n",
    "There is another way to code a time trend using the time series functionality of `dynlm`, with a call to `trend()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spurious_reg3 <- dynlm(divorceme ~ margarine + trend(spurious_ts), data = spurious_ts)\n",
    "summary(spurious_reg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560655a",
   "metadata": {},
   "source": [
    "What appears to have happened here is that the beginning year is reset to be $t = 1$. As a check, we can examine how the previous results compare to what we see here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecad120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lm()\n",
    "-47.28692 + 1999*0.02510\n",
    "# using dynlm()\n",
    "2.86153 + 0.02510"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ea532",
   "metadata": {},
   "source": [
    "Hm. Well, close enough I suppose!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372712f6",
   "metadata": {},
   "source": [
    "Some other things we can quickly and easily explore using `dynlm()` are whether looking at first differences might help. Consider again the model in levels, with a time trend:\n",
    "\n",
    "$$ \n",
    "divorceme_t = \\alpha + \\beta \\ margarine_t + \\delta \\ t + \\epsilon_t\n",
    "$$\n",
    "\n",
    "If this is true, then lagging the equation by one period should produce:\n",
    "\n",
    "$$ \n",
    "divorceme_{t-1} = \\alpha + \\beta \\ margarine_{t-1} + \\delta \\ (t - 1) + \\epsilon_{t-1}\n",
    "$$\n",
    "\n",
    "and then we might subtract the second equation from the first, and use the $\\Delta$ operator to denote the time difference. Several terms cancel, and we end up with:\n",
    "\n",
    "$$ \n",
    "\\Delta divorceme_t = \\delta + \\beta \\ \\Delta margarine_t + \\nu_t\n",
    "$$\n",
    "\n",
    "where $\\nu_t \\equiv \\epsilon_t - \\epsilon_{t-1}$. It is not clear whether we have gained anything by doing this, but it is probably worth examining. With `dynlm()`, we can quickly run this model using the `d()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "spurious_reg4 <- dynlm(d(divorceme) ~ d(margarine), data = spurious_ts)\n",
    "summary(spurious_reg4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e491b",
   "metadata": {},
   "source": [
    "But again, this doesn't help remove the hilariously persistent spurious correlation! May you melt a million deaths, national margarine consumption!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df985031",
   "metadata": {},
   "source": [
    "<h2>Serial correlation</h2>\n",
    "\n",
    "Another problem is that there is <b>serial correlation</b> in the error term. Serial correlation occurs when consecutive error terms, like $\\epsilon_t$ and $\\epsilon_{t+1}$, are systematically related to one another. Although we did not discuss it much or at all, in a well designed study like a randomized controlled trial, the error terms are well behaved, meaning they are independent and identically distributed. When they are serially correlated, they are not.\n",
    "\n",
    "In all of our models here, we can see the (positive) serial correlation. It shows up as consecutive errors that are above or below the time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(spurious$year, spurious_reg2$resid)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653cbf33",
   "metadata": {},
   "source": [
    "And here is the serial correlation in our model 4, with differenced $x$ and $y$ variables. Jiminy Christmas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(spurious$year[2:12], spurious_reg4$resid)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa7871",
   "metadata": {},
   "source": [
    "Does this all seem hopeless? We certainly seem trapped in a prison made out of margarine, with buttery fetters. But some takeaways are as follows:\n",
    "\n",
    "1. Be skeptical of time-series regressions\n",
    "2. Examine first differences and/or percentage changes. This is also no silver bullet\n",
    "3. Plot residuals\n",
    "4. If we knew about a <i>change</i> in something, driven by policy or weather, that might be useful to examine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d5957",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <span style=\"font-family:Papyrus; \">And they lived happily ever after. The End.</span></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
